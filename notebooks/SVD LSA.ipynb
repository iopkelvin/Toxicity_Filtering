{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocess_data.pickle', mode='rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', )\n",
    "doc_word = vectorizer.fit_transform(data.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "lsa = TruncatedSVD(6)\n",
    "doc_topic = lsa.fit_transform(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54835222, -0.09420743, -0.19779496, -0.49278545,  0.73468204,\n",
       "        -0.17121666],\n",
       "       [ 0.37868436, -0.09196782, -0.1179289 , -0.04671575, -0.13856657,\n",
       "        -0.08015258],\n",
       "       [ 0.0746036 , -0.02692589, -0.01835017,  0.01356787, -0.01372297,\n",
       "        -0.00953306],\n",
       "       ...,\n",
       "       [ 0.17940367, -0.00383929, -0.04304589, -0.06485946, -0.11664095,\n",
       "        -0.09302545],\n",
       "       [ 0.12072848,  0.00921115, -0.02272837,  0.00190658, -0.04602469,\n",
       "        -0.05118958],\n",
       "       [ 0.70836077, -0.21339635, -0.33750188, -0.2828425 , -0.34001704,\n",
       "         0.65461358]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.60612775e-04,  1.70666869e-04,  2.77136745e-06, ...,\n",
       "         1.72826514e-06,  3.63400046e-06,  1.91586500e-06],\n",
       "       [-6.88591870e-05, -5.32831559e-05,  1.32182161e-06, ...,\n",
       "         7.05179456e-06,  1.39604938e-05,  7.05624036e-06],\n",
       "       [-2.50090174e-04, -1.87614789e-04, -2.79858270e-06, ...,\n",
       "         6.15579057e-07,  1.26264871e-06,  4.73239973e-07],\n",
       "       [-1.66595359e-04, -2.35460793e-05,  8.83627033e-07, ...,\n",
       "         1.26704428e-07,  5.83307035e-07,  1.49094487e-08],\n",
       "       [-2.76442550e-04, -9.97296788e-06, -3.78539164e-06, ...,\n",
       "         2.05043582e-06,  4.90828566e-06,  1.74812723e-06],\n",
       "       [-3.38234812e-04, -7.10224692e-05, -4.40770104e-06, ...,\n",
       "        -1.45465775e-06, -2.22447709e-06, -1.71069640e-06]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaaa</th>\n",
       "      <th>aaaaaaaa</th>\n",
       "      <th>aaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaa</th>\n",
       "      <th>...</th>\n",
       "      <th>ğ™¬ğ™–ğ™¨</th>\n",
       "      <th>ğ™¬ğ™–ğ™©ğ™šğ™§ğ™¨</th>\n",
       "      <th>ğ™¬ğ™šğ™§ğ™š</th>\n",
       "      <th>ğ™¬ğ™ğ™¡ğ™¡</th>\n",
       "      <th>ğ™¬ğ™¤ğ™§ğ™ ğ™ğ™£ğ™œ</th>\n",
       "      <th>ğ™¬ğ™¤ğ™§ğ™¡ğ™™</th>\n",
       "      <th>ğ™¬ğ™§ğ™¤ğ™£ğ™œ</th>\n",
       "      <th>ğ™®ğ™šğ™–ğ™§ğ™¨</th>\n",
       "      <th>ğ™®ğ™¤ğ™ª</th>\n",
       "      <th>ğ™®ğ™¤ğ™ªğ™§</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.00066</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.00005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>-0.00025</td>\n",
       "      <td>-0.00019</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>-0.00017</td>\n",
       "      <td>-0.00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_5</th>\n",
       "      <td>-0.00028</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_6</th>\n",
       "      <td>-0.00034</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 283991 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  aa      aaa  aaaa  aaaaa  aaaaaa  aaaaaaaa  aaaaaaaaa  \\\n",
       "component_1  0.00066  0.00017   0.0    0.0     0.0   0.00001        0.0   \n",
       "component_2 -0.00007 -0.00005   0.0   -0.0    -0.0   0.00006        0.0   \n",
       "component_3 -0.00025 -0.00019  -0.0    0.0    -0.0   0.00001       -0.0   \n",
       "component_4 -0.00017 -0.00002   0.0    0.0    -0.0   0.00002        0.0   \n",
       "component_5 -0.00028 -0.00001  -0.0   -0.0    -0.0   0.00001       -0.0   \n",
       "component_6 -0.00034 -0.00007  -0.0   -0.0    -0.0   0.00001       -0.0   \n",
       "\n",
       "             aaaaaaaaaa  aaaaaaaaaaa  aaaaaaaaaaaaaa  ...  ğ™¬ğ™–ğ™¨  ğ™¬ğ™–ğ™©ğ™šğ™§ğ™¨  ğ™¬ğ™šğ™§ğ™š  \\\n",
       "component_1         0.0          0.0             0.0  ...  0.0     0.0   0.0   \n",
       "component_2        -0.0         -0.0             0.0  ... -0.0    -0.0  -0.0   \n",
       "component_3        -0.0          0.0            -0.0  ... -0.0    -0.0  -0.0   \n",
       "component_4         0.0          0.0             0.0  ... -0.0    -0.0  -0.0   \n",
       "component_5         0.0         -0.0             0.0  ... -0.0     0.0  -0.0   \n",
       "component_6        -0.0          0.0            -0.0  ... -0.0    -0.0  -0.0   \n",
       "\n",
       "                ğ™¬ğ™ğ™¡ğ™¡  ğ™¬ğ™¤ğ™§ğ™ ğ™ğ™£ğ™œ  ğ™¬ğ™¤ğ™§ğ™¡ğ™™  ğ™¬ğ™§ğ™¤ğ™£ğ™œ    ğ™®ğ™šğ™–ğ™§ğ™¨      ğ™®ğ™¤ğ™ª     ğ™®ğ™¤ğ™ªğ™§  \n",
       "component_1  0.00000      0.0    0.0    0.0  0.00000  0.00000  0.00000  \n",
       "component_2  0.00001     -0.0   -0.0   -0.0  0.00001  0.00001  0.00001  \n",
       "component_3  0.00000     -0.0   -0.0   -0.0  0.00000  0.00000  0.00000  \n",
       "component_4  0.00000      0.0    0.0   -0.0  0.00000  0.00000  0.00000  \n",
       "component_5  0.00000     -0.0   -0.0    0.0  0.00000  0.00000  0.00000  \n",
       "component_6 -0.00000     -0.0   -0.0   -0.0 -0.00000 -0.00000 -0.00000  \n",
       "\n",
       "[6 rows x 283991 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(5),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\", \"component_4\", \"component_5\", \"component_6\"],\n",
    "             columns = vectorizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "people, trump, like, just, did, does, time, think, know, right\n",
      "\n",
      "Topic  1\n",
      "trump, president, obama, donald, clinton, hillary, election, did, media, news\n",
      "\n",
      "Topic  2\n",
      "people, trump, white, black, president, donald, racist, homeless, hate, guns\n",
      "\n",
      "Topic  3\n",
      "tax, state, government, oil, pay, money, trump, income, taxes, alaska\n",
      "\n",
      "Topic  4\n",
      "like, tax, trump, oil, income, state, pay, taxes, people, sounds\n",
      "\n",
      "Topic  5\n",
      "just, tax, trump, people, income, pay, taxes, oil, money, sales\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "doc_word = vectorizer.fit_transform(data.comment_text)\n",
    "pd.DataFrame(doc_word.toarray(), index=ex_label, columns=vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(6)\n",
    "doc_topic = nmf_model.fit_transform(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\", \"component_4\", \"component_5\", \"component_6\"],\n",
    "             columns = vectorizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_model, vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "import random\n",
    "# data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer,sent_tokenize, word_tokenize \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import torch\n",
    "import warnings \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, add\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks,Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
